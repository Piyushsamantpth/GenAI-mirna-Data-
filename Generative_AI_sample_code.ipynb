{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/piyushsamant/Data/MicroRNA cohort data by Jin u for Generative AI/Experiment with Ridge Model small miRNA set\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, make_scorer, accuracy_score\n",
    "from sklearn.linear_model import SGDClassifier, ElasticNetCV, RidgeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split, KFold, cross_val_score\n",
    "from sklearn.datasets import make_classification\n",
    "from keras.initializers import RandomNormal\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, Embedding, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras import backend\n",
    "from keras.backend import mean\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defineing the CGan Model and training function in a single class\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, Embedding, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.backend import mean\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend\n",
    "\n",
    "\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return mean(y_true * y_pred)\n",
    "\n",
    "\n",
    "class cGAN1():\n",
    "    def __init__(self, latent_dim = 32, out_shape = 24):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.out_shape = out_shape\n",
    "        self.num_classes = 2\n",
    "        # using Adam as our optimizer\n",
    "\n",
    "        # Experiment with optimizer, learning rate, Beta1 \n",
    "        # optimizer = Adam(0.00001, 0.5)\n",
    "        optimizer = Adam(0.00001, 0.9)\n",
    "        # optimizer = SGD(lr = 0.0001, momentum = 0.5, nesterov = True)\n",
    "\n",
    "        # building the discriminator\n",
    "        self.discriminator = self.discriminator()\n",
    "        \n",
    "        # self.discriminator.compile(loss=['binary_crossentropy'], optimizer = optimizer,metrics=['accuracy'])\n",
    "        self.discriminator.compile(loss = wasserstein_loss, optimizer = optimizer, metrics = ['accuracy'])\n",
    "\n",
    "        # building the generator\n",
    "        self.generator = self.generator()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,))\n",
    "        gen_samples = self.generator([noise, label])\n",
    "\n",
    "        # we don't train discriminator when training generator\n",
    "        self.discriminator.trainable = False\n",
    "        valid = self.discriminator([gen_samples, label])\n",
    "\n",
    "        # combining both models\n",
    "        self.combined = Model([noise, label], valid)\n",
    "        \n",
    "        # self.combined.compile(loss=['binary_crossentropy'], optimizer=optimizer, metrics=['accuracy'])\n",
    "        self.combined.compile(loss = wasserstein_loss, optimizer = optimizer, metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "        self.G_losses = []\n",
    "        self.D_losses = []\n",
    "\n",
    "    def generator(self):\n",
    "        init = RandomNormal(mean=0.0, stddev=0.02)\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(128, input_dim=self.latent_dim))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "\n",
    "        model.add(Dense(256))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "\n",
    "        model.add(Dense(512))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "\n",
    "        model.add(Dense(self.out_shape, activation='tanh'))\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n",
    "\n",
    "        model_input = multiply([noise, label_embedding])\n",
    "        gen_sample = model(model_input)\n",
    "\n",
    "        return Model([noise, label], gen_sample, name=\"Generator\")\n",
    "\n",
    "    def discriminator(self):\n",
    "        init = RandomNormal(mean=0.0, stddev=0.02)\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(512, input_dim=self.out_shape, kernel_initializer=init))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Dense(256, kernel_initializer=init))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "\n",
    "        model.add(Dense(128, kernel_initializer=init))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "\n",
    "        model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "        gen_sample = Input(shape=(self.out_shape,))\n",
    "        label = Input(shape=(1,), dtype = 'int32')\n",
    "        label_embedding = Flatten()(Embedding(self.num_classes, self.out_shape)(label))\n",
    "\n",
    "        model_input = multiply([gen_sample, label_embedding])\n",
    "        validity = model(model_input)\n",
    "\n",
    "        return Model(inputs=[gen_sample, label], outputs=validity, name = \"Discriminator\")\n",
    "\n",
    "    def train(self, X_train, y_train, pos_index, neg_index, epochs, sampling=False, batch_size = 32, sample_interval = 100, plot=True):\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            if sampling:\n",
    "                idx1 = np.random.choice(pos_index, 8)\n",
    "                idx0 = np.random.choice(neg_index, batch_size - 8)\n",
    "                idx = np.concatenate((idx1, idx0))\n",
    "            else:\n",
    "                idx = np.random.choice(len(y_train), batch_size)\n",
    "            samples, labels = X_train[idx], y_train[idx]\n",
    "            samples, labels = shuffle(samples, labels)\n",
    "\n",
    "            # Sample noise as generator input\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            gen_samples = self.generator.predict([noise, labels])\n",
    "\n",
    "            if epoch < epochs // 1.5:\n",
    "                valid_smooth = (valid + 0.1) - (np.random.random(valid.shape) * 0.1)\n",
    "                fake_smooth = (fake - 0.1) + (np.random.random(fake.shape) * 0.1)\n",
    "            else:\n",
    "                valid_smooth = valid\n",
    "                fake_smooth = fake\n",
    "\n",
    "            # Train the discriminator\n",
    "            self.discriminator.trainable = True\n",
    "            d_loss_real = self.discriminator.train_on_batch([samples, labels], valid_smooth)\n",
    "            d_loss_fake = self.discriminator.train_on_batch([gen_samples, labels], fake_smooth)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # Train Generator\n",
    "            self.discriminator.trainable = False\n",
    "            sampled_labels = np.random.randint(0, 2, batch_size).reshape(-1, 1)\n",
    "            g_loss = self.combined.train_on_batch([noise, sampled_labels], valid)\n",
    "\n",
    "            if (epoch + 1) % sample_interval == 0:\n",
    "                print('[%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f' % (epoch, epochs, d_loss[0], g_loss[0]))\n",
    "            self.G_losses.append(g_loss[0])\n",
    "            self.D_losses.append(d_loss[0])\n",
    "\n",
    "            if plot:\n",
    "                if epoch + 1 == epochs:\n",
    "                    plt.figure(figsize=(3, 2))\n",
    "                    plt.title(\"Generator and Discriminator Loss\")\n",
    "                    plt.plot(self.G_losses, label=\"G\")\n",
    "                    plt.plot(self.D_losses, label=\"D\")\n",
    "                    plt.xlabel(\"iterations\")\n",
    "                    plt.ylabel(\"Loss\")\n",
    "                    plt.legend()\n",
    "                    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No of miRNAs need to be consider\n",
    "no_of_miRNA = 24\n",
    "# no of epoches in the experiment\n",
    "iteration = 1500\n",
    "\n",
    "# Calling C Gan in actial code \n",
    "cgan = cGAN1(out_shape = no_of_miRNA)\n",
    "\n",
    "y_outer_train_1 = y_train.reshape(-1,1)  # X_train and y_train are ndarray of feature and target of the training  \n",
    "pos_index = np.where( y_train_1 == 1)[0]\n",
    "neg_index = np.where( y_train_1 == 0)[0]\n",
    "cgan.train(X_train, y_train, pos_index, neg_index, epochs = iteration) # iteration is typically more than 1000 but can change accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once Model is trained and we have obesrved the G and D Losses can generate new datapoints\n",
    "\n",
    "# Defining a random noise according to the no of new samples generated\n",
    "noise = np.random.normal(0, 1, (1000, 32))\n",
    "        \n",
    "# To generate 500 instances with class value 0 \n",
    "sampled_labels = np.zeros(1000).reshape(-1, 1)\n",
    "gen_samples = cgan.generator.predict([noise, sampled_labels])\n",
    "gen_df_zeros = pd.DataFrame(data = gen_samples, columns = X_pd.columns)  # X_pd is the input dataframe with mirnas as columns\n",
    "gen_df_zeros['Class'] = 0\n",
    "        \n",
    "# To generate 500 instances with class value 1 \n",
    "sampled_labels_2 = np.ones(1000).reshape(-1, 1)\n",
    "gen_samples_2 = cgan.generator.predict([noise, sampled_labels_2])\n",
    "gen_df_ones = pd.DataFrame(data = gen_samples_2, columns = X_pd.columns)\n",
    "gen_df_ones['Class'] = 1\n",
    "\n",
    "\n",
    "# Combining the generated datasets for zero and ones\n",
    "\n",
    "combine_df_generated = pd.concat([gen_df_ones, gen_df_zeros], ignore_index=True, sort=False)\n",
    "combine_df_generated = combine_df_generated.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "X_generated = combine_df_generated.drop('Class', axis = 1)\n",
    "y_generated = combine_df_generated['Class']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
